---
title: "kNN"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

## kNN: k optimization

```{r, include=FALSE}
### Create test and training sets
library(readr)
heart.df = read.csv("~/R/STA 141A/heart_2020.csv", header = T, stringsAsFactors = T)

heart.df.split = split(heart.df, heart.df$HeartDisease)
perc.of.data = 0.05
prop = nrow(heart.df.split$No) / nrow(heart.df)
val = floor(perc.of.data * nrow(heart.df))
x = floor(prop * val)
y = val - x
set.seed(1)
No.x = sample(1:nrow(heart.df.split$No), size =  x, replace = F)
Yes.y = sample((1:nrow(heart.df.split$Yes))[-(No.x)], size = y, replace=F)
heart.test = rbind(heart.df.split$No[No.x,], heart.df.split$Yes[Yes.y,])
heart.training = rbind(heart.df.split$No[-No.x,], heart.df.split$Yes[-Yes.y,])
```

This is the function for calculating the optimal k to use. Due to the long computing time (about 5 hours thanks to Connor), there will be no output shown here.

```{r, eval=FALSE}
library(class)
knn_k_misclass = function(k_test){
  knn_output = knn(train = heart.training.adj, test = heart.test.adj,

library(class)
knn_k_misclass = function(k_test){
  knn_output = knn(train = heart.training.adj, test = heart.test.adj,  
                   cl = heart.training.adj$HeartDisease, k = k_test, use.all = T)
  confusion_knn = t(table(knn_output, heart.test$HeartDisease, dnn = c("Predicted","True")))
  misclassification_rate_knn = 1 - (sum(diag(confusion_knn)) / sum(confusion_knn))
  return(misclassification_rate_knn)
}
min_misclass_rate = Inf
misclass_rates = numeric()
optimal_k = NULL
start_time = proc.time()

n=sqrt(nrow(heart.training))/2

for(i in 1:ceiling(n)){
  cur_misclass_rate = knn_k_misclass(i)
  misclass_rates[i] = cur_misclass_rate
  if(cur_misclass_rate < min_misclass_rate){
    min_misclass_rate = cur_misclass_rate
    optimal_k = i
  }
}
optimal_k
```

To determine what the optimal k value to use for our kNN algorithms is, we ran an exhaustive search to compare the misclassification rates of ks from 1 to 277. By doing so, we found the optimal k value for our data to be k=3, which resulted in a misclassification rate of 0.0529. This rate is noticeably lower when compared to the misclassifcation rate when using the general practice k value of sqrt(n)/2 = 275.5 which was 0.0852. Thus, we find that the general practice k has worse performance than the optimal k. Therefore, if using an algorithm that can find the optimal k efficiently, doing so is recommended over using the simpler general practice k value.

## k Optimization Graphs
```{r, eval=FALSE}
library(ggplot2)
#optimal_k_output = 3
#misclass.df = as.data.frame(cbind(k = 1:276, Misclass_Rate=misclass_rates))
#write.csv(misclass.df, "C:\\File Path\\knn_misclass_rates.csv", row.names=F)
misclass.df = read.csv("knn_misclass_rates.csv", header=T)

misclass_plot = ggplot(data = misclass.df, aes(x=k, y=Misclass_Rate))+
  geom_line(col="red2")+
  geom_point(data = misclass.df[-which.min(misclass.df$Misclass_Rate),], col="dodgerblue3", alpha=0.5)+
  geom_point(data = misclass.df[which.min(misclass.df$Misclass_Rate),], col = "green", alpha = 0.5, size=2.5)+
  labs(x = "k Value", y = "Misclassification Rate")+
  scale_x_continuous(breaks = seq(from = 0, to = 276, by = 50))+
  scale_y_continuous(breaks = seq(from = 0.05, to = 0.09, by = 0.005))

accuracy_plot = ggplot(data = misclass.df, aes(x=k, y= (1 - Misclass_Rate)*100))+
  geom_line(col="red2")+
  geom_point(data = misclass.df[-which.min(misclass.df$Misclass_Rate),], col="dodgerblue3", alpha=0.5)+
  geom_point(data = misclass.df[which.min(misclass.df$Misclass_Rate),], col = "green", alpha = 0.5, size=2.5)+
  labs(x = "k Value", y = "Accuracy")+
  scale_x_continuous(breaks = seq(from = 0, to = 276, by = 50))+
  ylim(90,95)

misclass_plot
accuracy_plot
```

### Misclassification Plot
![](images/misclass_plot.png)
The misclassification error rate increases as the k value increases.

### Accuracy Plot
![](images/accuracy_plot.png)
The accuracy decreases as the k value increases.

## Confusion Misclassification Function

This function is used to compute the confusion matrix of the knn full and no medical models as well as the full, no medical, and medical logistic regression models.

```{r, eval=FALSE}
confusion_misclass = function(data, model, conf_mat = F, err_rate = T, threshold = 0.5, knn=F){
  if(knn == F){
    prob = predict(model, newdata = data, type = "response")
    predicted = ifelse(prob > threshold, "Yes", "No")

    confusion = table(predicted, as.character(data$HeartDisease),
                      dnn = c("Predicted","True"))

    misclassification_rate = 1 - (sum(diag(confusion)) / sum(confusion))

    confusion = t(confusion)
    
  } else {
    confusion = t(table(model, data$HeartDisease, dnn = c("Predicted","True")))
    colnames(confusion) = c("No", "Yes")
    misclassification_rate = 1 - (sum(diag(confusion)) / sum(confusion))
    
  }

  result = list()
  
  if(conf_mat == T){
    result$Confusion_Matrix = confusion
  }
  
  if(err_rate == T){
    result$Misclassification_Rate = misclassification_rate
  }
  
  return(result)
}
```

To determine what impacts prior medical conditions and diseases have on whether a person will be predicted to develop heart disease, we will be comparing the misclassification rates of several different algorithms and models. We will be comparing the results of a kNN algorithm that uses all predictors and a kNN algorithm that uses all predictors except the predictors for prior medical conditions. Additionally, we will be comparing the results of a logistic regression using all of the predictors, a logistic regression using all of the predictors except the predictors for prior medical conditions, and a logistic regression using only the predictors for prior medical conditions. We will not be using a kNN algorithm based on only the predictors for prior medical conditions because due to the low number of factors and large sample size, this results in there being far too many ties (each possible observation will have ~5000 ties) so the kNN algorithm will not be usable.

## kNN: Full
```{r, echo=FALSE, eval=FALSE}
## kNN: Full
library(class)
heart.test.adj = heart.test
heart.training.adj = heart.training
indx.test = sapply(heart.test.adj, is.factor)
heart.test.adj[indx.test] = lapply(heart.test.adj[indx.test], FUN=as.numeric)
indx.training = sapply(heart.training.adj, is.factor)
heart.training.adj[indx.training] = lapply(heart.training.adj[indx.training], FUN=as.numeric)
### General Practice: k = sqrt(n) / 2
n = sqrt(nrow(heart.training)) / 2
knn_output_n = knn(train = heart.training.adj, test = heart.test.adj, 
                 cl = heart.training.adj$HeartDisease, k = n, use.all = T)
confusion_knn_n = confusion_misclass(data = heart.test, model = knn_output_n, 
                                     knn = T, conf_mat = T)
confusion_knn_n
### Optimal k: k = 3
knn_output_3 = knn(train = heart.training.adj, test = heart.test.adj, 
                 cl = heart.training.adj$HeartDisease, k = 3, use.all = T)
confusion_knn_3 = confusion_misclass(data = heart.test, model = knn_output_3, 
                                     knn = T, conf_mat = T)
confusion_knn_3
```

### Confusion matrix for knn full, k = n
![](images/knn_full_n.png)

### Confusion matrix for knn full, k = 3
![](images/knn_full_3.png)

## kNN: Without Medical 
```{r, echo=FALSE, eval=FALSE}
med_ind = c(5, 12, 16, 17, 18)
heart.test.adj.2 = heart.test.adj[-med_ind]
heart.training.adj.2 = heart.training.adj[-med_ind]
### General Practice: k = sqrt(n) / 2
n = sqrt(nrow(heart.training)) / 2
knn_output_med_n = knn(train = heart.training.adj.2, 
                       test = heart.test.adj.2, 
                       cl = heart.training.adj.2$HeartDisease, k = n, use.all = T)
confusion_knn_med_n = confusion_misclass(data = heart.test.adj.2, model = knn_output_med_n, 
                                         conf_mat = T, knn = T)
confusion_knn_med_n
### Optimal k: k = 3
knn_output_med_3 = knn(train = heart.training.adj.2, test = heart.test.adj.2, 
                 cl = heart.training.adj$HeartDisease, k = 3, use.all = T)
confusion_knn_med_3 = confusion_misclass(data = heart.test.adj.2, model = knn_output_med_3,
                                         conf_mat = T, knn=T)
confusion_knn_med_3
```

### Confusion matrix for knn no medical, k = n
![](images/knn_nomedical_n.png)

### Confusion matrix for knn no medical, k = 3
![](images/knn_nomedical_3.png)

## Table
```{r, echo=FALSE, eval=FALSE}
#full log model
logreg_full = glm(HeartDisease~., data=heart.training, family = "binomial")
confusion_full = confusion_misclass(data = heart.test, model = logreg_full, conf_mat = T)
confusion_full

#log model medical only
logreg_only_med = glm(HeartDisease~Stroke+Diabetic+Asthma+KidneyDisease+SkinCancer,
                      data=heart.training, family = "binomial")
confusion_misclass_med = confusion_misclass(data = heart.test, model = logreg_only_med)
confusion_misclass_med

#log model without medical
heart.test.no.med = heart.test[-med_ind]
heart.training.no.med = heart.training[-med_ind]
logreg_no_med = glm(HeartDisease~.,
                      data=heart.training.no.med, family = "binomial")
confusion_misclass_no_med = confusion_misclass(data = heart.test.no.med, model = logreg_no_med)
confusion_misclass_no_med
```

### Confusion matrix for the full logistic regression model
![](images/log_full.png)

```{r, eval=FALSE}
library(kableExtra)
tests = c("kNN: Full", "kNN: w/o Medical", 
                      "LogReg: Full", "LogReg: w/o Medical", "LogReg: Only Medical")
miss.rates = c(confusion_knn_3[[2]], confusion_knn_med_3[[2]], confusion_full[[2]], 
               confusion_misclass_no_med[[1]], confusion_misclass_med[[1]])
misclass.rate.df = data.frame(Model = tests, 
                              Misclassification_Rates = miss.rates)
misclass.rate.df %>% 
  kbl(caption = "Misclassification Rate by Model") %>% 
  kable_classic(full_width = F, html_font = "Cambria") 
```

### Table of the misclassification rates by all models
![](images/misclassrate.png)

When running the kNN algorithm (with k=3) using all of the predictors, we had a misclassification rate of 0.0529. The kNN algorithm (with k=3) without the predictors for prior medical conditions, resulted in a smaller misclassification rate of 0.0439. However, the logistic regression using all of the predictors had a misclassification rate of 0.0823 and the logistic regression not using the predictors for prior medical conditions had a misclassification rate of 0.0851. Lastly, the logistic regression which only used the predictors for prior medical conditions had a misclassification rate of 0.0852.

Based on these results, we see that the exclusion of the predictors for prior medical conditions decreased the misclassification rate when using kNN, but increased the misclassification rate when using logistic regression. However, since these differences were so slight (<0.01), we conclude that the exclusion of prior medical conditions had no significant effect on the effectiveness of the algorithms and models. However, we can also see that the logistic regression model that only considered prior medical conditions had a misclassification rate only 0.0019 larger than the full model. Thus, we find that a model that only uses prior medical conditions is almost as effective, if not as effective as the full model. Therefore, we conclude that prior medical conditions have a significant impact on whether an individual may develop heart disease.